Section 2: Choosing the right Database

Non Functional Requirements are impacted by the choice of Databases.

Caching Solution -> Redis, Memcache, etc. 

When we want to store Videos/Images: We would require Blob Storage. Ex. Amazon S3. 

CDN is used for distributing a same file (Images, Videos, etc) in geographically different locations. 

Hence for Blob Storage, we would be using an S3 along with CDN. 

For Search Capability, we can use ElasticSearch and Solr. Both are built on top of Apache Lucene. 

We must incorporate Fuzzy search capability for Elastic Search and Solr so that even if an user
mispels certain keywords like airport as airprot, the search can identify the correct word.

The difference between a Database and Search Engine is that Databases guarantees that data will not
be lost, but Search Engine does not guarantee any such things. Hence the primary source of truth
should be Databases and we can load the data into the search engines for searching capabilities. 

Time Series Databases are similar to Relational Databases, but here in TS DBs, random updates does
not take place. It's more kind of append only database. Also read query in TS DBs are bulk read
queries spread across a time range. 

Example of Time Series Database: Influx DB, Open DB. 

When we want to do analytics on Transactional Databases, we use Datawarehouses. It provides
inhouse capabilities for Queries by aggregating on large transactions. 

For Databases which will provide us ACID guarantees (ex. in a bank transactions one account's debit
should be equal to one account's credit), we should relational Databases like MySQL, Oracle,
SQL server, Posgres. 

For Databases where we need wide variety of datatypes and characteries for individual attributes, we 
should chose Document DB. Ex. MongoDB and CouchBase. 

For ever Increasing data, but with limited Quering capabilities w.r.t. Document DBs and whose
ACID properties may not be guaranteed, we can chose Columnar DataBases. Ex. Cassandra, HBase, etc. 

For Complex Scenarios where there are ever increasing transactions as well as where ACID properties
needs to be preserved, we must use combination of Columnar DBs and RBDMS. 

Section 3: System Design Case Studies - Practical Real world applications

Topic 1: URL Shortener System Design

Functional Requirements: 
    - Get Short URL
    - Redirect to Long URL
Non Functional Requirements:
    - Very Low Latency
    - Very High Availability

Number of Unique Request the system can handle in 10 years considering it recieves X requests per
second:
 X*60*60*24*365*10 = Y Total Requests

What all characters to consider for the url: 
Here we consider : a-z, A-Z and 0-9 = 26+26+10 = 62 Alphabets

We then need to consider, how long will our URL be:
If we Choose one length, we can only have 62 URLs, 
If we Chose two length, we can chose 62*62=62^2 characters
Similarly, if we chose n length, we can chose 62^n characters

We have to opt for a length such that 62^n > Y
OR, n > log(62)Y

If we consider 2000 request per seconds, then 
Y=X*60*60*24*365*10=2000*60*60*24*365*10=630720000000 requests in 10 years

Hence, n> log(62)630720000000
Hence n>6.58.

Hence Minimum Length of the URL shortener will be 7.  

Maximum URL that can be supported is 62^7=3521614606208=3.5 T URLs. 

If we are using two or multiple servers to generate the Short URL, the two server can generate the 
same short url and this may lead to collision. 

Hence to Counter this problem, these multiple URL shortener servers can be the client to Redis
Server which can provide the URL Shorteners to the client URL shortening service and auto increment
its count. This will have one major problem: The Redis Server here will become the single point of
failure. 

The alternate to the above problem is keep two (or more) redis server, but make sure the url-shortener
keys that these multiple redis server stores are distinct from one another. One URL Shortener will be 
connected to few URL shortener servers, while the other URL Shortener will be connected to the 
remaining URL shortener servers. This way there will not be any point of failures. 

What if we need to add more redis servers, than we might need to again auto allocate and distribute the
URL shortening keys to the new redis server. Hence we can totally leave aside redis and we can auto
distribute the token numbers with specific range to each URL Shortener servers. These URL shortener 
servers will directly connect to token generation service which will provide the tokens based on
the token range assigned to the URL shortener servers. This new Token Server will be a simple mysql 
database. Once one key is utilized, it will be marked as taken, and that url can never be provided 
again. 

The token service must have multiple replicas, so that it does not become a single point of failures. 

One problem of the token service is that, incase one of the URL shortener service which is assigned 
range of tokens, dies down while being operational, then in that case we will be loosing out on the 
tokens which might go for ever. For simplicity purpose, we don't require to keep track of those tokens
which are lost. 

For retrieval of the longer url by providing the short url, the short url service takes the short
url, it then takes the information from the backend cassandra database and gives back the long 
url back to the client requesting for the long url.

Cassandra is preferred because it has to store 3+ trillion records. 

At present no other information is being captured:
    - From which location is the request coming for generating the Short URL
    - From what kind of device is the request coming, etc. 

To log analytics, KAFKA based asynchronous topics can be created which will capture the relevant 
information and store it in some database for visualization purpose. 

The downside of using Kafka asynchronous is there might not be few non captured events, and since
it is a non financial logs, few misses are fine. 

Another downside is Using Kafka for capturing every logs which involves CPU and Memory. What we can 
do in this case is we can create queuing mechanism, such that incase the queue crosses certain 
thresholds, we can then aggregate the logs and flush them all at once into Kafka Topic. This way, 
the IO needed while utilizing KAFKA greatly reduces as KAFKA calls will happen N//M times where 
M is the number of records captured in the queue at a given time. It is thus more batch than 
real time.

Topic 2: Airbnb / Booking.com System Design

Functional Requirements:
    - Hotel
        - Onboarding
        - updates
        - Bookings
    - User
        - Search
        - Book
        - Check Bookings

Non Functional Requirements:
    - Low Latency
    - High Availability
    - High Consistency
    - Scale:
        - 500 K hotels
        - 10 M rooms
        - 1000 rooms/hotel

The Detailed Workflow:
    - UI App for hotel room owners to add and update rooms. The rooms details will be stored in 
    MYSQL database (Information DB). The Images of the hotels will be stored in CDNs
    - UI App for users to search and book hotels.
        - The search service is powered by Elastic Search Cluster which sends out a KAFKA pull
        request to the MYSQL Databases (Information DB and Rooms Availability DB). The search
        should have fuzzy capabilities. 
        - The booking service should be able to retrieve the data from the Rooms Availability
        MySQL DB and update the Database. The Terminal Booking state, i.e., cancelled bookings and 
        past bookings are stored in a Cassandra DB with partition on UserIDs
    - UI App for Management Service to get a view on current bookings by pulling from the 
    Rooms Availability DB through Redis (so as not to put more load on the Avalability DB) and
    a Cassandra DB which contains historic imformation on users bookings and Cancellation Status.
    - For Analytics, Spark Streaming can be utilized directly from Kafka. For Machine Learning, train
    from Cassandra DB and Predict using Kafka and Spark Streaming.

    DataBase and HTTPs Information:
        POST /hotels: Add new hotels
        GET /hotels/id: Get information about existing hotels
        PUT /hotels/id: Update Hotel related Information
        PUT /hotels/id/rooms/id: Update Information about Hotel's room 
    
    Hotel DB: 
        hotel - [id, name, locality-id, description, original_images,display_images,is_active]
        hotel-facilities - [id, hotel_id, facilty_id, is_active]
        rooms - [id, hotel_id, display_name, is_active, quantity, price_min, price_max]
        rooms_facilities - [id, room_id, facility_id,is_active]
        facilities - [id, display_name]
        locality - [id, city_id, state_id, country_id, zip_code, is_active]

    Booking DB: 
        Available rooms  - [room_id, date, initial_qty, available_qty], constrains available_qty>0
        Booking - [booking_id, room_id, user_id, start_date, end_date, no_of_rooms, status, 
                invoice_id]. Caveat: One room_id for one_booking
        Status are: [Reserved, Booked, cancelled, completed]
        
    For Booking a Room:
        - A post request will be sent with the user_id, room_id, quantity, start_date
        and end_date 
        - Check in available rooms
        - Insert in booking and reduce the available_qty field in the available rooms 
        - Put in redis with TTL to check whether payment has been carried out before key
            expiry. Incase Payment is a failure, then we need to unblock the room with 
            status  reserved to cancelled and we should increment the available_qty to 
            its earlier state. Incase the payment is a success before key expiry, then 
            we need to book the room from reserve and generate an invoice ID. Here 
            redis call back is used for record updation. 
    
We must distribute the database as per geography. We can create a primary replica
which will have a backup replica for each region. Users who are near to a specific 
region gets access to the data for those regions for ticket bookings. 

Topic: Amazon System Design (Design an Ecommerce Application)

Functional Requirements:
    - Search
    - Cart/ Wish List
    - Checkout
    - View Orders

Non Functional Requirements:
    - Low Latency
    - High Availability
    - High Consistency

High Level Design Information:
    - An Inbound Service communicates with the sellers about any new product characteristics
    - This Inbound Service is fed through Kafka. 
    - There will be an Item Service which stores the details of new item and stores it in 
    MongoDB. It acts like the Item DB 
    - When ever an User search for a specific information, it comes through a User Search Page. 
    The items are stored in an Elastic Search and are retrieved through a search service. 
    - The Search Service is connected to a Serviceability + TAT service which provides the
    item delivery time information and also whether the item can be delivered to that specific 
    location. 
    - The Search service is also connected through to a Cart Service and a Wish List Service. Both
    are connected to individual MYSQL DB. 
    - The search service is also used for Analytics as the information flows through Kafka with
    userID and ItemID information. 
    - The Analytics service will run a Spark Streaming Job and run Machine Learning Algorithms 
    over a Spark Cluster and they in turn would populate the recommended items in the User's 
    home page.
    - The User Service provides the details about the user to both the search service and the 
    User Home Page. It sits on Top of a SQL DB and connected through Redis. A query is made 
    to redis related to the details of the user to the Redis. If the details is not found, it is
    directly queries to the MYSQL DB and the user details are fetched through the User Service and 
    populated in the Home Page and Search page. 
    - The Warehouse service and Logistics service are called by the Serviceability + TAT service to
    calculate the shortest path and time.

For the User Purchase Flow, it will communicate to a Order Taking Service which will use a
SQL Database to store the order information. When an order is moved for payment, the order Taking
service (OTS) would capture the information in the SQL would call another service  - Inventory Service
(IS) and its associated MYSQL DB and thereby decrease the count. It will wait for the communication
from the Payment service. If the payment service confirms the payment, the mysql connected to the
order taking service will be updated to Order Confirmed. Incase the payment is a failure, then the
SQL DB for OTS will be updated to cancelled and the SQL DB for IS will be updated as per the count
of cancelled order. In-case no communication is received from the payment gateway, a redis server
which is communicating with the MYSQL DB of OTS will signal a timeout and the DB will get updated 
to cancelled followed by DB update in IS. 

Continuing the above case, the normal case is payment update takes place first followed by 
a Redis Time Out. To combat this scenario, as soon as the payment update takes place, the 
associated record may be deleted from Redis. The other scenario happpens when the 
payment update happens later than the the Redis TimeOut. Here OTS would be updated to cancelled 
state and the IS will be updated to the earlier count. In this case, either the amount is
refunded to the user, or the same order is placed for the user once the payment update is 
received. 

The Redis timeout generally happens tad later than the declared time. It is okay in this case. 

The MYSQL db which preserves ACID properties, can be prone to bottleneck. To combat this problem, 
the Orders which has reached their terminal state (order delivered or return days has passed)
can be transferred to a Cassandra DB through an Archival Service. The Archival Service calls
an Order Processing Service for fetching the records which have reached the terminal state. The
records when received is transferred to the Historical order service and this Service populates
the data in Cassandra. The same data is then deleted in MYSQL through the Order Processing Service. 
The Archival Service is a Cron Setup which will run at a specified duration.

To view pervious and current orders, the Orders View Service will call both the Order Processing
Service (to call the MYSQL db) and the Historical Order Service (to call the Cassandra DB) to 
merge the present and the historical orders and thereby Showcase to the User.

Once an Order is placed, it triggers a Notification Service which alerts the Customer and the
sellers. 

The orders which are placed, are sent to KAFKA and are used for Spark Streaming. The Spark 
Streaming will run a Machine Learning for Recommendation and will populate the recommended
next set of buys for the users when they logs in.

Topic: Chat Application System Design. Ex. Whatsapp, Facebook, Messenger, etc 

Functional Requirements:
    - One to One Chat
    - Group Chat
    - Send and Receive Text/ Images/ Videos  
    - Read Receipt
    - Last Seen

Non Funtional Requirements:
    -  2 Billion users
    - 1.6 Billion Monthly Active Users
    - 65 Billion Messages per day


High Level Design:
    - Each user sends message to other users through a message ID and through a certain port 
    in a server called Web Socket Handler. Each machine has roughly 65K ports, and out of which
    if 5k ports are for internal usage, then we have 60K ports for allocating to individual users
    per machine. 
    - The websocket handler connects to a Web Socket Manager which stores the information about 
    the devices of the sending user, sender's and receiver's Web Socket Handler details and 
    also list of users connected to the a particular web socket. 
    
    - A Redis Server sits on top of this Web Socket Manager, the Redis server which gets updated once the 
    data flows into the Web Socket Manager. 

    - The Web Socket Handler also connects to a Message Services which stores the information to a 
    Cassandra DB which scales preety well.

    - When a User wants to send a message, its corresponding web socket handler sends the information 
    to Web Socket Manager (WSM), the sending party retreives the web socket handler information 
    details of the receiving party from the WSM and sends the messages directly to the web socket 
    controller of the receiving party. And there by bi-directional message flow is established through 
    a web socket. 

    - When a new user tries to post a message to another new user, Redis Server is queried to check 
    whether both the new users are part of the same Web Socket Handler. If yes, then the Web Socker 
    Manager is not queried and it redices the latency. 

    - The cache time is kept at a significantly less duration as the devices of the users connected
    to a particular Web Socket Handler might change very frequently. 

    - The messages which one user sends to another needs to be stored atleast temporarily to a 
    Cassandra Database through Message Services. 

    - Incase a sender sends a message to a receiver, but the receiver is not connected to any socket 
    handler, then the messages with relevant details flows into the Cassandra DB through message service. 
    Now after sometime once the user comes online, it queries the Message Service regarding any Messages
    it had recieved along with the senders Socket Handler details, and there by it starts a web socket 
    connection with the senders Socket Handler.

    - There can be a scenario when the Message is sent but the receiver just happens to have come online
    and before the the Socket Manager could identify the receiver's corresponding socket handler's
    information and store the information in Cassandra DB (once it finds out that the receivers web 
    socket does not exists), the reciever query the Database and finds out that no message has been 
    received. To tackle this kind of a scenario, the web socket handler of the receiver can establish a 
    Long Polling with the messaging service and get the message only once it receives from the 
    message service. Alternately the receiver's socket handler can make multiple requests to the message
    services. 

    - Incase an User is offline, but still sends some messages. In this situation, the messages gets 
    stored to the users local device db and once the user comes online, the user is connected to a 
    specific socket handler and the message is transferred from the user's local db to the messaging 
    system server. 

    - For Group Messaging Service, the message is not trasferred through a Web Socket Handler, which 
    is more oriented for one-on-one messaging. Once a service is made to a group, it is directly 
    sent to the messaging service the messaging service sends the message and group information to 
    a KAFKA topic which is pulled by Group Message Handler Service. The Group Message Handler after
    receiving the details of the group and the sender, queries the Group Service to fetch the associated
    members in the group except for the sender. These senders' details are then queried through the 
    Web Socket Manager and their Web Socket Handlers are identified and the messages are delivered to 
    the individuals through the Web Socket Handler.

    - For sharing Images or Videos, the Images are shared to the Asset Service, which encrypts the image,
    stores it in a S3 bucket (CDN can also be applied to these images) and shares the https link with the 
    socket handler and then this socket handler transfers the image address to the recipient through Web 
    Socket Connectivity.  

    - During Sports events or Political Events, one image/videos can be uploaded multiple times. 
    In order to let go on storing of duplicate images, we use multiple hashing algorithms on the image
    and validate whether the hash values already exists in the db. If it exists, then don't upload and 
    just provide the existing link, else upload and provide the existing link. 

The User App will have the Following services:
    - User Service: Responsible for storing and fetching all the user related information. Sits on top 
    of a MYSQL user DB and is connected to Redis for caching.
    - Group Service: Responsible for storing and fetching information related to group and its members. 
    Like a User Service, this too sits on top of a MYSQL DB and is connected to Redis for caching.
    - Analytics Service: The chat information of a user is sent via Kafka for Analytics service probably
    through Spark Streaming. 
    - Last Seen Service: The last seen service takes the users last seen status through KAFKA (above) 
    and Populates it in Cassandra Database. Since the throughput is very high, we use a Cassandra DB and 
    not MySQL. 

For Monitoring of various DBs, KAFKA latencies and others, we can use Grafana. 

Topic: Notification System at Scale

Functional Requirements:
    - Send Notifications
    - Plugable
    - SaaS
    - Prioritization

Non Functional Requirements:
    - High Availability
    - Many Clients

The High Level Design is like:
    - For SAAS, what kind of message it is used for sending, email, generic etc is taken for 
    consideration
    - For internal usage of the notification service, the priorities will be embedded. 
    - Once messages are received from various clients it is sent to KAFKA. 
    - Before proceeding farther, a Notification Validator and prioritizer checks whether the conditions
    are being met, like recipients are not null, etc. Also User Transaction Data is also validated to be 
    Not Null.
    - Once the Validation is done, the messages passes through a rate limiter/request counter service
    through another KAFKA based service. The rate limiter service checks the number of messages passing 
    at a specified duration. It uses redis in the backend to increment the counter. The Request Counter 
    Service checks the number of requests coming from the subscription service. It does not put a limit 
    on the rate, bit it auto-increments the bill per requests made. This too uses a redis in the backend 
    for increasing the request count. 
    - After the rate limiter/request counter service, comes the Notification Handler and User
    Preference Service which limits the number of requests we can make to a particular user and
    also which users to communicate. It uses MySQL DB in its backsend. 
    - Once the above stages passes, the notification message is passed through another KAFKA topic:
        - SMS Handler: For Handling SMS along with its Regional Vendors
        - Email Handler: For Handling Emails along with its Regional Vendors
        - In-App Handler: For handling messages in Databases Like Firebases, Apple Push, etc
        - IVRS Handler: For Handling COD related Messages. Requires Low Throughput.
    - All the messages which is flowing through the Notification Handler and User
    Preference Service will be sent to a Notification Tracker Service which would populate the
    message in a Cassandra DB. Incase some one sues or during auditing, this database
    will be handy for exploring all notifications that were ever sent. 
    - Incase of order which needs to be sent in Bulk, ex. pass the message to the consumers who
    have purchased a TV in the last 24 hours, or purchased milk for repurchase, the message once
    it passes the Notification Handler and User Preference Service, the KAFKA topic also connects 
    with the Transaction Data Parser Service which after parsing the information, stores the info 
    in Elastic Search or MongoDB, etc. This db is used by Query Engine for performing multiple 
    actions like a. Rule Engine (suspend users who frequently cancels orders), b. Fraud Detection, 
    c. Search Platform. By utilizing the Searh Platform, a Bulk Notification service is performed and 
    the message is sent to the Notification Service. 


Topic: Ola/Uber System Design

Functional Requirements:
    - See Cabs
    - ETA & approx Price
    - Book a Cab
    - Location Tracking

Non Functional Requirements: 
    - Global
    - Low Latency
    - High Availability
    - High Consistency
    - Scale
        - 100 Million Users
        - 10 Million rides per day

A grid style segment is created where Cabs are tracked and Cab's Location are continously
changing and hence the tracking has to be done real time. 

The Map Service does the Segment Management (Grid) service and allocate the number of 
drivers to be present in a particular location and also should two or more segments be 
merged based on traffic patterns. 

High Level Design:
    - The User App connects to a User Service through a Load Balancer. This User Service is 
    responsible for connecting to UserDB and Redis and updating the profile information of 
    the user along with other user details. 
    - For Booking a Cab, a Cab Request Service talks with the Cab Finder Service and through
    a web socket, the User App books the Cab. 
    - A driver connects through a Driver App via the Driver Service. The Driver Service provides
    all the details regarding the driver, the payment information among others. The Driver 
    Service connects to a Driver MYSQL DB at the backend powered by redis. 
    - The Driver App also connects to a Location Service which gives the location details of 
    the drivers so that the Map Service can utilize the same and aid the Cab Finder. 
    
    - The Drivers Connects to multiple services via Web Socket Handlers (Servers) and the 
    websocket handler:driver mapping is stored in WebSocket Manager. The same is also
    stored in a redis cluster. The Redis Cluster contains the mapping of d:h, i.e, which
    driver gets connected to which Socket Handler. 
    - The Drivers:WebSocket Handler Connects to the Location Service as well which
    populates the data in a Cassandra DB such that it can store an ever growing information 
    of the driver's location. The Location Service is also connected to the Maps Service 
    and the map service gives the segment:driver location which is passed to the Location 
    Service and it stores the data in a Redis DataBase. 
    - The Trip Service Provides Trip related information of each drivers and users. All the 
    latest trip or ongoing trip information is stored in a MySQL DB and the historical trip 
    information is stored in Cassandra DB. To populate the historical trip information from 
    MYSQL to Cassandra, we use a Trip Archiver Service, which uses a Cron Service at the
    backend to transfer the data.
    - The Customer's journey starts with the Cab Service Requests. The Cab Service Request
    queries the Cab Finder which in turn queries the Location Service. The Location Service 
    in turn queries the Map Service and the Map Service takes the Customers Latitude and 
    Longitute and searches the Driver on a certain segment and its adjoining segment and provides
    a list of available drivers back to the Cab Finder Service. The cab finder service in turn
    queries a Driver Priority Engine to assign a suitable driver for the customer. If the 
    customer is a premium customer, than the Driver Priority Engine would provide the best Driver
    of the lot. If it is a normal customer, then the Driver Priority Engine will Broadcast the 
    Drivers about the Customer, and the Driver who selects the customer at the earliest, is
    auto assigned the driver. Now the Driver host is fetched from the WebSocket Manager, where
    Driver:Host combination is stored. The Driver is contacted via the host, and upon the approval
    of the driver, the Customer is notified of the driver and his/her details.
    - The Messages from Driver Priority Engine, Cab Finder Service and Location Service is 
    transferred to KAFKA for performing Analytics. 
    - The Kafka cluster can be used to aggregate the payment information of the drivers and the 
    payment calculation can happen over a MYSQL. 
    - Also to mitigate the No Driver Found Issue, the Analytics can communicate the Drivers 
    regarding sparse Driver Zone. Also Spark ML jobs can run to feed the Driver Priority Engine and 
    Customer Clssification (Premium or Regular), Driver Profiling and User Profiling. 
    - The Analytics can also run a fraud detection algorithm, where in if a driver books a
    drive from his own car, then it should be marked as fraudulent based on the consistent latitude
    and longitude of the customer's contact and the driver's contact. 
    - The Analytics can also power the map service by predicting the quality of the road, by 
    calculating the average speed of other company cars in the same road.

Topic: Twitter System Design 

Functional Requirements:
    - Tweet
    - Re-Tweet
    - Follow: Unidirectional. User A follows User B does not mean that User B will follow
    User A. 
    - Search: Search Capability on trending events. 

Non-Functional Requirements:
    - Read Heavy - More People sees tweet than post
    - Fast Rendering - Tweets should be able to be rendered to the read user pretty fast. 
    - Fast Tweet - Tweets should be promptly uploaded
    - Lag Okay in some cases - The Tweets can be taken some time to be visible to the users who
    are reading it. 
    - Scale 
        - 150 Million Daily active users
        - 350 Million Monthly active users
        - 1.5 Billion Accounts
        - 500 Million Tweets per day
            - Turns out to 5700 tweets/sec
            - 12000 Tweets/sec during peek hours.

Type of Users Classified in Twitter:
    - Famous Users: Celebrities, Politicians, etc
    - Active Users: Users who have used Twitter in the last 3 days
    - Live Users: Users who are presently accessing Twitter
    - Passive Users: Users who are neither celebrities nor active users
    - Inactive Users: Users who have gone through soft delete state.

User Onboarding Design:
    - The User Onboarding and Login happens through a User Service. Any User/Users information
    is fetched through the User Service. The User Service sits on top of a MySQL user db. The 
    choice of DB is SQL because even though the user size is large, it will still be finite. 
    There is also a Redis Cache connected to the User Service. Incase the information about 
    a certain user is fetched, it will first search the redis, and if information is not found, 
    then it will do a read operation on the SQL database. 
    - Redis will have the following details:
        - User Details
        - Followers
        - Following
        - Type
        - Last Access Time
    - A User Follow API is connected through a Load Balancer to a Graph Service which will
    have the logic to find the Followers and Following of a Particualr User. The Graph Service
    sits on top of a MySQL Cluster which is sharded and also connects to a Redis Cluster. When
    ever a request is made through an UI, the Graph Service first queries the Redis DB and tries
    to find the followers and following. If the redis does not have the information stored, 
    then it queries the sharded MYSQL DB. 
    - The Third Service which can be used is to provide Analytics on users (users who spends time
    more on a particular tweet). The repsonse from the analytics service can be sent to KAFKA
    - For Live Users, User Live WebSockets (Notifications) are created, and once the User becomes
    Active, the information is sent through KAFKA to the User Service and the status of that
    user is subsequently updated. 
    
Tweet Design:
    - When an User tries to post a tweet, it passes through a Load Balancer and hits the Tweet
    Ingestion Service (TIS). The TIS is used only for POST based Call.
    - Incase the Tweet contains multimedia files then it will be passed through an Asset Service
    and incase the Tweet contains a Long URL (Max Tweet Length is 140 Characters), it will
    pass through a URL shortening Service. 
    - The Tweets that are processed by the Tweet Ingestion Service sits on top of a Cassandra 
    DB. Cassandra is chosen due the volume of the tweets which are collected by Twitter each day. 
    - The Tweet that is getting stored in the Cassandra DB contains the following information:
        - Tweet ID
        - User ID who posts the tweet
        - Tweet Content
    - The TIS also puts the tweet to a KAFKA for further analytics. 
    - To get the user's tweet content/ create time lines of users, a Tweet Service is used. 
    This gets the details from the Cassandra DB and populates the relevant information.
    - The User TimeLine UI gives all the tweets/ retweets the user has made through the Time line
    Service
    - The Home TimeLine UI gives all the tweets/ retweets of the ID who the user follows through 
    the timeline service.
    -  The TimeLine service gets the input from the Tweet Processor which in turn gets the input
    from the KAFKA which gets the inputs from the Tweet Ingestion Service. Once the tweet is 
    received by the Tweet Processor, it finds out the the followers of the user by querying to the
    Graph Service who made the tweet and thereby populates the timeline service which will then work 
    to populate the Home TimeLine of the Followers. 
    - The Redis being an in-memory database, only handles the tweets for the live or to certain
    extent, the active users. But incase a Passive User comes online, then the Timeline Service
    queries the User Service and the Graph Service first. It gets all the ids the passive User
    follows, then it proceeds to the Tweet Service to query the ids and the tweets over a 
    timeline and consequently poulates the timelines for the Passive Users while storing the 
    tweets in the redis.
    - For the Live users, once they are already on the app, further query from redis will not
    take place. Alternately the Tweet ingestion service will send messages to Kafka and that will
    create a live web-socket notification and that will directly populate in the user's app. 

Search Design:
    - The Tweet Ingestion Service communicates with KAFKA and that in turn communicates
    to a Search Consumer Service(write) which after transformation stores it in Elastic Search DB.
    - When a Search for a particular tweet is made, it communicates with a Search Service(read) and 
    that in turn fetches the information from the Elastic Search. 
    - For a trending search, it is prudent to cache the search in redis and not search Elastic 
    Search all the time. Hence a search once made is kept in redis for certain amount of time, 
    and the same subsequent search when arriving at the search service is queried directly 
    from redis and not Elastic Search again.

Analytics Design:
    - The Tweet Ingestion Service communicates with KAFKA and that in turn communicates
    to a Spark streaming Service. It can extract the top 10 words that have been used in the last
    n hours (after removing stop words) and find out the trending words.
    - The databases that can be used here is redis. The list of trending words can be extracted
    from redis to a User's UI. 
    - The trends can also be Geographic. 
    - For Passive Users, customized popular tweets can be made as a part of cron and sent out
    to those users, so that they can start tweeting again. This will use a notification 
    service which takes the information from the user service and sends the email to the 
    recipients. 




