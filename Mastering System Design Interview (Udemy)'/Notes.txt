Section 2: Choosing the right Database

Non Functional Requirements are impacted by the choice of Databases.

Caching Solution -> Redis, Memcache, etc. 

When we want to store Videos/Images: We would require Blob Storage. Ex. Amazon S3. 

CDN is used for distributing a same file (Images, Videos, etc) in geographically different locations. 

Hence for Blob Storage, we would be using an S3 along with CDN. 

For Search Capability, we can use ElasticSearch and Solr. Both are built on top of Apache Lucene. 

We must incorporate Fuzzy search capability for Elastic Search and Solr so that even if an user
mispels certain keywords like airport as airprot, the search can identify the correct word.

The difference between a Database and Search Engine is that Databases guarantees that data will not
be lost, but Search Engine does not guarantee any such things. Hence the primary source of truth
should be Databases and we can load the data into the search engines for searching capabilities. 

Time Series Databases are similar to Relational Databases, but here in TS DBs, random updates does
not take place. It's more kind of append only database. Also read query in TS DBs are bulk read
queries spread across a time range. 

Example of Time Series Database: Influx DB, Open DB. 

When we want to do analytics on Transactional Databases, we use Datawarehouses. It provides
inhouse capabilities for Queries by aggregating on large transactions. 

For Databases which will provide us ACID guarantees (ex. in a bank transactions one account's debit
should be equal to one account's credit), we should relational Databases like MySQL, Oracle,
SQL server, Posgres. 

For Databases where we need wide variety of datatypes and characteries for individual attributes, we 
should chose Document DB. Ex. MongoDB and CouchBase. 

For ever Increasing data, but with limited Quering capabilities w.r.t. Document DBs and whose
ACID properties may not be guaranteed, we can chose Columnar DataBases. Ex. Cassandra, HBase, etc. 

For Complex Scenarios where there are ever increasing transactions as well as where ACID properties
needs to be preserved, we must use combination of Columnar DBs and RBDMS. 

Section 3: System Design Case Studies - Practical Real world applications

Topic 1: URL Shortener System Design

Functional Requirements: 
    - Get Short URL
    - Redirect to Long URL
Non Functional Requirements:
    - Very Low Latency
    - Very High Availability

Number of Unique Request the system can handle in 10 years considering it recieves X requests per
second:
 X*60*60*24*365*10 = Y Total Requests

What all characters to consider for the url: 
Here we consider : a-z, A-Z and 0-9 = 26+26+10 = 62 Alphabets

We then need to consider, how long will our URL be:
If we Choose one length, we can only have 62 URLs, 
If we Chose two length, we can chose 62*62=62^2 characters
Similarly, if we chose n length, we can chose 62^n characters

We have to opt for a length such that 62^n > Y
OR, n > log(62)Y

If we consider 2000 request per seconds, then 
Y=X*60*60*24*365*10=2000*60*60*24*365*10=630720000000 requests in 10 years

Hence, n> log(62)630720000000
Hence n>6.58.

Hence Minimum Length of the URL shortener will be 7.  

Maximum URL that can be supported is 62^7=3521614606208=3.5 T URLs. 

If we are using two or multiple servers to generate the Short URL, the two server can generate the 
same short url and this may lead to collision. 

Hence to Counter this problem, these multiple URL shortener servers can be the client to Redis
Server which can provide the URL Shorteners to the client URL shortening service and auto increment
its count. This will have one major problem: The Redis Server here will become the single point of
failure. 

The alternate to the above problem is keep two (or more) redis server, but make sure the url-shortener
keys that these multiple redis server stores are distinct from one another. One URL Shortener will be 
connected to few URL shortener servers, while the other URL Shortener will be connected to the 
remaining URL shortener servers. This way there will not be any point of failures. 

What if we need to add more redis servers, than we might need to again auto allocate and distribute the
URL shortening keys to the new redis server. Hence we can totally leave aside redis and we can auto
distribute the token numbers with specific range to each URL Shortener servers. These URL shortener 
servers will directly connect to token generation service which will provide the tokens based on
the token range assigned to the URL shortener servers. This new Token Server will be a simple mysql 
database. Once one key is utilized, it will be marked as taken, and that url can never be provided 
again. 

The token service must have multiple replicas, so that it does not become a single point of failures. 

One problem of the token service is that, incase one of the URL shortener service which is assigned 
range of tokens, dies down while being operational, then in that case we will be loosing out on the 
tokens which might go for ever. For simplicity purpose, we don't require to keep track of those tokens
which are lost. 

For retrieval of the longer url by providing the short url, the short url service takes the short
url, it then takes the information from the backend cassandra database and gives back the long 
url back to the client requesting for the long url.

Cassandra is preferred because it has to store 3+ trillion records. 

At present no other information is being captured:
    - From which location is the request coming for generating the Short URL
    - From what kind of device is the request coming, etc. 

To log analytics, KAFKA based asynchronous topics can be created which will capture the relevant 
information and store it in some database for visualization purpose. 

The downside of using Kafka asynchronous is there might not be few non captured events, and since
it is a non financial logs, few misses are fine. 

Another downside is Using Kafka for capturing every logs which involves CPU and Memory. What we can 
do in this case is we can create queuing mechanism, such that incase the queue crosses certain 
thresholds, we can then aggregate the logs and flush them all at once into Kafka Topic. This way, 
the IO needed while utilizing KAFKA greatly reduces as KAFKA calls will happen N//M times where 
M is the number of records captured in the queue at a given time. It is thus more batch than 
real time.

Topic 2: Airbnb / Booking.com System Design

Functional Requirements:
    - Hotel
        - Onboarding
        - updates
        - Bookings
    - User
        - Search
        - Book
        - Check Bookings

Non Functional Requirements:
    - Low Latency
    - High Availability
    - High Consistency
    - Scale:
        - 500 K hotels
        - 10 M rooms
        - 1000 rooms/hotel

The Detailed Workflow:
    - UI App for hotel room owners to add and update rooms. The rooms details will be stored in 
    MYSQL database (Information DB). The Images of the hotels will be stored in CDNs
    - UI App for users to search and book hotels.
        - The search service is powered by Elastic Search Cluster which sends out a KAFKA pull
        request to the MYSQL Databases (Information DB and Rooms Availability DB). The search
        should have fuzzy capabilities. 
        - The booking service should be able to retrieve the data from the Rooms Availability
        MySQL DB and update the Database. The Terminal Booking state, i.e., cancelled bookings and 
        past bookings are stored in a Cassandra DB with partition on UserIDs
    - UI App for Management Service to get a view on current bookings by pulling from the 
    Rooms Availability DB through Redis (so as not to put more load on the Avalability DB) and
    a Cassandra DB which contains historic imformation on users bookings and Cancellation Status.
    - For Analytics, Spark Streaming can be utilized directly from Kafka. For Machine Learning, train
    from Cassandra DB and Predict using Kafka and Spark Streaming.

    DataBase and HTTPs Information:
        POST /hotels: Add new hotels
        GET /hotels/id: Get information about existing hotels
        PUT /hotels/id: Update Hotel related Information
        PUT /hotels/id/rooms/id: Update Information about Hotel's room 
    
    Hotel DB: 
        hotel - [id, name, locality-id, description, original_images,display_images,is_active]
        hotel-facilities - [id, hotel_id, facilty_id, is_active]
        rooms - [id, hotel_id, display_name, is_active, quantity, price_min, price_max]
        rooms_facilities - [id, room_id, facility_id,is_active]
        facilities - [id, display_name]
        locality - [id, city_id, state_id, country_id, zip_code, is_active]

    Booking DB: 
        Available rooms  - [room_id, date, initial_qty, available_qty], constrains available_qty>0
        Booking - [booking_id, room_id, user_id, start_date, end_date, no_of_rooms, status, 
                invoice_id]. Caveat: One room_id for one_booking
        Status are: [Reserved, Booked, cancelled, completed]
        
    For Booking a Room:
        - A post request will be sent with the user_id, room_id, quantity, start_date
        and end_date 
        - Check in available rooms
        - Insert in booking and reduce the available_qty field in the available rooms 
        - Put in redis with TTL to check whether payment has been carried out before key
            expiry. Incase Payment is a failure, then we need to unblock the room with 
            status  reserved to cancelled and we should increment the available_qty to 
            its earlier state. Incase the payment is a success before key expiry, then 
            we need to book the room from reserve and generate an invoice ID. Here 
            redis call back is used for record updation. 
    
We must distribute the database as per geography. We can create a primary replica
which will have a backup replica for each region. Users who are near to a specific 
region gets access to the data for those regions for ticket bookings. 

Topic: Amazon System Design (Design an Ecommerce Application)

Functional Requirements:
    - Search
    - Cart/ Wish List
    - Checkout
    - View Orders

Non Functional Requirements:
    - Low Latency
    - High Availability
    - High Consistency

High Level Design Information:
    - An Inbound Service communicates with the sellers about any new product characteristics
    - This Inbound Service is fed through Kafka. 
    - There will be an Item Service which stores the details of new item and stores it in 
    MongoDB. It acts like the Item DB 
    - When ever an User search for a specific information, it comes through a User Search Page. 
    The items are stored in an Elastic Search and are retrieved through a search service. 
    - The Search Service is connected to a Serviceability + TAT service which provides the
    item delivery time information and also whether the item can be delivered to that specific 
    location. 
    - The Search service is also connected through to a Cart Service and a Wish List Service. Both
    are connected to individual MYSQL DB. 
    - The search service is also used for Analytics as the information flows through Kafka with
    userID and ItemID information. 
    - The Analytics service will run a Spark Streaming Job and run Machine Learning Algorithms 
    over a Spark Cluster and they in turn would populate the recommended items in the User's 
    home page.
    - The User Service provides the details about the user to both the search service and the 
    User Home Page. It sits on Top of a SQL DB and connected through Redis. A query is made 
    to redis related to the details of the user to the Redis. If the details is not found, it is
    directly queries to the MYSQL DB and the user details are fetched through the User Service and 
    populated in the Home Page and Search page. 
    - The Warehouse service and Logistics service are called by the Serviceability + TAT service to
    calculate the shortest path and time.

For the User Purchase Flow, it will communicate to a Order Taking Service which will use a
SQL Database to store the order information. When an order is moved for payment, the order Taking
service (OTS) would capture the information in the SQL would call another service  - Inventory Service
(IS) and its associated MYSQL DB and thereby decrease the count. It will wait for the communication
from the Payment service. If the payment service confirms the payment, the mysql connected to the
order taking service will be updated to Order Confirmed. Incase the payment is a failure, then the
SQL DB for OTS will be updated to cancelled and the SQL DB for IS will be updated as per the count
of cancelled order. In-case no communication is received from the payment gateway, a redis server
which is communicating with the MYSQL DB of OTS will signal a timeout and the DB will get updated 
to cancelled followed by DB update in IS. 

Continuing the above case, the normal case is payment update takes place first followed by 
a Redis Time Out. To combat this scenario, as soon as the payment update takes place, the 
associated record may be deleted from Redis. The other scenario happpens when the 
payment update happens later than the the Redis TimeOut. Here OTS would be updated to cancelled 
state and the IS will be updated to the earlier count. In this case, either the amount is
refunded to the user, or the same order is placed for the user once the payment update is 
received. 

The Redis timeout generally happens tad later than the declared time. It is okay in this case. 

The MYSQL db which preserves ACID properties, can be prone to bottleneck. To combat this problem, 
the Orders which has reached their terminal state (order delivered or return days has passed)
can be transferred to a Cassandra DB through an Archival Service. The Archival Service calls
an Order Processing Service for fetching the records which have reached the terminal state. The
records when received is transferred to the Historical order service and this Service populates
the data in Cassandra. The same data is then deleted in MYSQL through the Order Processing Service. 
The Archival Service is a Cron Setup which will run at a specified duration.

To view pervious and current orders, the Orders View Service will call both the Order Processing
Service (to call the MYSQL db) and the Historical Order Service (to call the Cassandra DB) to 
merge the present and the historical orders and thereby Showcase to the User.

Once an Order is placed, it triggers a Notification Service which alerts the Customer and the
sellers. 

The orders which are placed, are sent to KAFKA and are used for Spark Streaming. The Spark 
Streaming will run a Machine Learning for Recommendation and will populate the recommended
next set of buys for the users when they logs in.

Topic: Chat Application System Design. Ex. Whatsapp, Facebook, Messenger, etc 

Functional Requirements:
    - One to One Chat
    - Group Chat
    - Send and Receive Text/ Images/ Videos  
    - Read Receipt
    - Last Seen

Non Funtional Requirements:
    -  2 Billion users
    - 1.6 Billion Monthly Active Users
    - 65 Billion Messages per day


High Level Design:
    - Each user sends message to other users through a message ID and through a certain port 
    in a server called Web Socket Handler. Each machine has roughly 65K ports, and out of which
    if 5k ports are for internal usage, then we have 60K ports for allocating to individual users
    per machine. 
    - The websocket handler connects to a Web Socket Manager which stores the information about 
    the devices of the sending user, sender's and receiver's Web Socket Handler details and 
    also list of users connected to the a particular web socket. 
    
    - A Redis Server sits on top of this Web Socket Manager, the Redis server which gets updated once the 
    data flows into the Web Socket Manager. 

    - The Web Socket Handler also connects to a Message Services which stores the information to a 
    Cassandra DB which scales preety well.

    - When a User wants to send a message, its corresponding web socket handler sends the information 
    to Web Socket Manager (WSM), the sending party retreives the web socket handler information 
    details of the receiving party from the WSM and sends the messages directly to the web socket 
    controller of the receiving party. And there by bi-directional message flow is established through 
    a web socket. 

    - When a new user tries to post a message to another new user, Redis Server is queried to check 
    whether both the new users are part of the same Web Socket Handler. If yes, then the Web Socker 
    Manager is not queried and it redices the latency. 

    - The cache time is kept at a significantly less duration as the devices of the users connected
    to a particular Web Socket Handler might change very frequently. 

    - The messages which one user sends to another needs to be stored atleast temporarily to a 
    Cassandra Database through Message Services. 

    - Incase a sender sends a message to a receiver, but the receiver is not connected to any socket 
    handler, then the messages with relevant details flows into the Cassandra DB through message service. 
    Now after sometime once the user comes online, it queries the Message Service regarding any Messages
    it had recieved along with the senders Socket Handler details, and there by it starts a web socket 
    connection with the senders Socket Handler.

    - There can be a scenario when the Message is sent but the receiver just happens to have come online
    and before the the Socket Manager could identify the receiver's corresponding socket handler's
    information and store the information in Cassandra DB (once it finds out that the receivers web 
    socket does not exists), the reciever query the Database and finds out that no message has been 
    received. To tackle this kind of a scenario, the web socket handler of the receiver can establish a 
    Long Polling with the messaging service and get the message only once it receives from the 
    message service. Alternately the receiver's socket handler can make multiple requests to the message
    services. 

    - Incase an User is offline, but still sends some messages. In this situation, the messages gets 
    stored to the users local device db and once the user comes online, the user is connected to a 
    specific socket handler and the message is transferred from the user's local db to the messaging 
    system server. 

    - For Group Messaging Service, the message is not trasferred through a Web Socket Handler, which 
    is more oriented for one-on-one messaging. Once a service is made to a group, it is directly 
    sent to the messaging service the messaging service sends the message and group information to 
    a KAFKA topic which is pulled by Group Message Handler Service. The Group Message Handler after
    receiving the details of the group and the sender, queries the Group Service to fetch the associated
    members in the group except for the sender. These senders' details are then queried through the 
    Web Socket Manager and their Web Socket Handlers are identified and the messages are delivered to 
    the individuals through the Web Socket Handler.

    - For sharing Images or Videos, the Images are shared to the Asset Service, which encrypts the image,
    stores it in a S3 bucket (CDN can also be applied to these images) and shares the https link with the 
    socket handler and then this socket handler transfers the image address to the recipient through Web 
    Socket Connectivity.  

    - During Sports events or Political Events, one image/videos can be uploaded multiple times. 
    In order to let go on storing of duplicate images, we use multiple hashing algorithms on the image
    and validate whether the hash values already exists in the db. If it exists, then don't upload and 
    just provide the existing link, else upload and provide the existing link. 

The User App will have the Following services:
    - User Service: Responsible for storing and fetching all the user related information. Sits on top 
    of a MYSQL user DB and is connected to Redis for caching.
    - Group Service: Responsible for storing and fetching information related to group and its members. 
    Like a User Service, this too sits on top of a MYSQL DB and is connected to Redis for caching.
    - Analytics Service: The chat information of a user is sent via Kafka for Analytics service probably
    through Spark Streaming. 
    - Last Seen Service: The last seen service takes the users last seen status through KAFKA (above) 
    and Populates it in Cassandra Database. Since the throughput is very high, we use a Cassandra DB and 
    not MySQL. 

For Monitoring of various DBs, KAFKA latencies and others, we can use Grafana. 

Topic: Notification System at Scale

Functional Requirements:
    - Send Notifications
    - Plugable
    - SaaS
    - Prioritization

Non Functional Requirements:
    - High Availability
    - Many Clients

The High Level Design is like:
    - For SAAS, what kind of message it is used for sending, email, generic etc is taken for 
    consideration
    - For internal usage of the notification service, the priorities will be embedded. 
    - Once messages are received from various clients it is sent to KAFKA. 
    - Before proceeding farther, a Notification Validator and prioritizer checks whether the conditions
    are being met, like recipients are not null, etc. Also User Transaction Data is also validated to be 
    Not Null.
    - Once the Validation is done, the messages passes through a rate limiter/request counter service
    through another KAFKA based service. The rate limiter service checks the number of messages passing 
    at a specified duration. It uses redis in the backend to increment the counter. The Request Counter 
    Service checks the number of requests coming from the subscription service. It does not put a limit 
    on the rate, but it auto-increments the bill per requests made. This too uses a redis in the backend 
    for increasing the request count. 
    - After the rate limiter/request counter service, comes the Notification Handler and User
    Preference Service which limits the number of requests we can make to a particular user and
    also which users to communicate. It uses MySQL DB in its backsend. 
    - Once the above stages passes, the notification message is passed through another KAFKA topic:
        - SMS Handler: For Handling SMS along with its Regional Vendors
        - Email Handler: For Handling Emails along with its Regional Vendors
        - In-App Handler: For handling messages in Databases Like Firebases, Apple Push, etc
        - IVRS Handler: For Handling COD related Messages. Requires Low Throughput.
    - All the messages which is flowing through the Notification Handler and User
    Preference Service will be sent to a Notification Tracker Service which would populate the
    message in a Cassandra DB. Incase some one sues or during auditing, this database
    will be handy for exploring all notifications that were ever sent. 
    - Incase of order which needs to be sent in Bulk, ex. pass the message to the consumers who
    have purchased a TV in the last 24 hours, or purchased milk for repurchase, the message once
    it passes the Notification Handler and User Preference Service, the KAFKA topic also connects 
    with the Transaction Data Parser Service which after parsing the information, stores the info 
    in Elastic Search or MongoDB, etc. This db is used by Query Engine for performing multiple 
    actions like a. Rule Engine (suspend users who frequently cancels orders), b. Fraud Detection, 
    c. Search Platform. By utilizing the Searh Platform, a Bulk Notification service is performed and 
    the message is sent to the Notification Service. 


Topic: Ola/Uber System Design

Functional Requirements:
    - See Cabs
    - ETA & approx Price
    - Book a Cab
    - Location Tracking

Non Functional Requirements: 
    - Global
    - Low Latency
    - High Availability
    - High Consistency
    - Scale
        - 100 Million Users
        - 10 Million rides per day

A grid style segment is created where Cabs are tracked and Cab's Location are continously
changing and hence the tracking has to be done real time. 

The Map Service does the Segment Management (Grid) service and allocate the number of 
drivers to be present in a particular location and also should two or more segments be 
merged based on traffic patterns. 

High Level Design:
    - The User App connects to a User Service through a Load Balancer. This User Service is 
    responsible for connecting to UserDB and Redis and updating the profile information of 
    the user along with other user details. 
    - For Booking a Cab, a Cab Request Service talks with the Cab Finder Service and through
    a web socket, the User App books the Cab. 
    - A driver connects through a Driver App via the Driver Service. The Driver Service provides
    all the details regarding the driver, the payment information among others. The Driver 
    Service connects to a Driver MYSQL DB at the backend powered by redis. 
    - The Driver App also connects to a Location Service which gives the location details of 
    the drivers so that the Map Service can utilize the same and aid the Cab Finder. 
    
    - The Drivers Connects to multiple services via Web Socket Handlers (Servers) and the 
    websocket handler:driver mapping is stored in WebSocket Manager. The same is also
    stored in a redis cluster. The Redis Cluster contains the mapping of d:h, i.e, which
    driver gets connected to which Socket Handler. 
    - The Drivers:WebSocket Handler Connects to the Location Service as well which
    populates the data in a Cassandra DB such that it can store an ever growing information 
    of the driver's location. The Location Service is also connected to the Maps Service 
    and the map service gives the segment:driver location which is passed to the Location 
    Service and it stores the data in a Redis DataBase. 
    - The Trip Service Provides Trip related information of each drivers and users. All the 
    latest trip or ongoing trip information is stored in a MySQL DB and the historical trip 
    information is stored in Cassandra DB. To populate the historical trip information from 
    MYSQL to Cassandra, we use a Trip Archiver Service, which uses a Cron Service at the
    backend to transfer the data.
    - The Customer's journey starts with the Cab Service Requests. The Cab Service Request
    queries the Cab Finder which in turn queries the Location Service. The Location Service 
    in turn queries the Map Service and the Map Service takes the Customers Latitude and 
    Longitute and searches the Driver on a certain segment and its adjoining segment and provides
    a list of available drivers back to the Cab Finder Service. The cab finder service in turn
    queries a Driver Priority Engine to assign a suitable driver for the customer. If the 
    customer is a premium customer, than the Driver Priority Engine would provide the best Driver
    of the lot. If it is a normal customer, then the Driver Priority Engine will Broadcast the 
    Drivers about the Customer, and the Driver who selects the customer at the earliest, is
    auto assigned the driver. Now the Driver host is fetched from the WebSocket Manager, where
    Driver:Host combination is stored. The Driver is contacted via the host, and upon the approval
    of the driver, the Customer is notified of the driver and his/her details.
    - The Messages from Driver Priority Engine, Cab Finder Service and Location Service is 
    transferred to KAFKA for performing Analytics. 
    - The Kafka cluster can be used to aggregate the payment information of the drivers and the 
    payment calculation can happen over a MYSQL. 
    - Also to mitigate the No Driver Found Issue, the Analytics can communicate the Drivers 
    regarding sparse Driver Zone. Also Spark ML jobs can run to feed the Driver Priority Engine and 
    Customer Clssification (Premium or Regular), Driver Profiling and User Profiling. 
    - The Analytics can also run a fraud detection algorithm, where in if a driver books a
    drive from his own car, then it should be marked as fraudulent based on the consistent latitude
    and longitude of the customer's contact and the driver's contact. 
    - The Analytics can also power the map service by predicting the quality of the road, by 
    calculating the average speed of other company cars in the same road.

Topic: Twitter System Design 

Functional Requirements:
    - Tweet
    - Re-Tweet
    - Follow: Unidirectional. User A follows User B does not mean that User B will follow
    User A. 
    - Search: Search Capability on trending events. 

Non-Functional Requirements:
    - Read Heavy - More People sees tweet than post
    - Fast Rendering - Tweets should be able to be rendered to the read user pretty fast. 
    - Fast Tweet - Tweets should be promptly uploaded
    - Lag Okay in some cases - The Tweets can be taken some time to be visible to the users who
    are reading it. 
    - Scale 
        - 150 Million Daily active users
        - 350 Million Monthly active users
        - 1.5 Billion Accounts
        - 500 Million Tweets per day
            - Turns out to 5700 tweets/sec
            - 12000 Tweets/sec during peek hours.

Type of Users Classified in Twitter:
    - Famous Users: Celebrities, Politicians, etc
    - Active Users: Users who have used Twitter in the last 3 days
    - Live Users: Users who are presently accessing Twitter
    - Passive Users: Users who are neither celebrities nor active users
    - Inactive Users: Users who have gone through soft delete state.

User Onboarding Design:
    - The User Onboarding and Login happens through a User Service. Any User/Users information
    is fetched through the User Service. The User Service sits on top of a MySQL user db. The 
    choice of DB is SQL because even though the user size is large, it will still be finite. 
    There is also a Redis Cache connected to the User Service. Incase the information about 
    a certain user is fetched, it will first search the redis, and if information is not found, 
    then it will do a read operation on the SQL database. 
    - Redis will have the following details:
        - User Details
        - Followers
        - Following
        - Type
        - Last Access Time
    - A User Follow API is connected through a Load Balancer to a Graph Service which will
    have the logic to find the Followers and Following of a Particular User. The Graph Service
    sits on top of a MySQL Cluster which is sharded and also connects to a Redis Cluster. When
    ever a request is made through an UI, the Graph Service first queries the Redis DB and tries
    to find the followers and following. If the redis does not have the information stored, 
    then it queries the sharded MYSQL DB. 
    - The Third Service which can be used is to provide Analytics on users (users who spends time
    more on a particular tweet). The reonse from the analytics service can be sent to KAFKA
    - For Live Users, User Live WebSockets (Notifications) are created, and once the User becomes
    Active, the information is sent through KAFKA to the User Service and the status of that
    user is subsequently updated. 
    
Tweet Design:
    - When an User tries to post a tweet, it passes through a Load Balancer and hits the Tweet
    Ingestion Service (TIS). The TIS is used only for POST based Call.
    - Incase the Tweet contains multimedia files then it will be passed through an Asset Service
    and incase the Tweet contains a Long URL (Max Tweet Length is 140 Characters), it will
    pass through a URL shortening Service. 
    - The Tweets that are processed by the Tweet Ingestion Service sits on top of a Cassandra 
    DB. Cassandra is chosen due the volume of the tweets which are collected by Twitter each day. 
    - The Tweet that is getting stored in the Cassandra DB contains the following information:
        - Tweet ID
        - User ID who posts the tweet
        - Tweet Content
    - The TIS also puts the tweet to a KAFKA for further analytics. 
    - To get the user's tweet content/ create time lines of users, a Tweet Service is used. 
    This gets the details from the Cassandra DB and populates the relevant information.
    - The User TimeLine UI gives all the tweets/ retweets the user has made through the Time line
    Service
    - The Home TimeLine UI gives all the tweets/ retweets of the ID who the user follows through 
    the timeline service.
    -  The TimeLine service gets the input from the Tweet Processor which in turn gets the input
    from the KAFKA which gets the inputs from the Tweet Ingestion Service. Once the tweet is 
    received by the Tweet Processor, it finds out the the followers of the user by querying to the
    Graph Service who made the tweet and thereby populates the timeline service which will then work 
    to populate the Home TimeLine of the Followers. 
    - The Redis being an in-memory database, only handles the tweets for the live or to certain
    extent, the active users. But incase a Passive User comes online, then the Timeline Service
    queries the User Service and the Graph Service first. It gets all the ids the passive User
    follows, then it proceeds to the Tweet Service to query the ids and the tweets over a 
    timeline and consequently poulates the timelines for the Passive Users while storing the 
    tweets in the redis.
    - For the Live users, once they are already on the app, further query from redis will not
    take place. Alternately the Tweet ingestion service will send messages to Kafka and that will
    create a live web-socket notification and that will directly populate in the user's app. 

Search Design:
    - The Tweet Ingestion Service communicates with KAFKA and that in turn communicates
    to a Search Consumer Service(write) which after transformation stores it in Elastic Search DB.
    - When a Search for a particular tweet is made, it communicates with a Search Service(read) and 
    that in turn fetches the information from the Elastic Search. 
    - For a trending search, it is prudent to cache the search in redis and not search Elastic 
    Search all the time. Hence a search once made is kept in redis for certain amount of time, 
    and the same subsequent search when arriving at the search service is queried directly 
    from redis and not Elastic Search again.

Analytics Design:
    - The Tweet Ingestion Service communicates with KAFKA and that in turn communicates
    to a Spark streaming Service. It can extract the top 10 words that have been used in the last
    n hours (after removing stop words) and find out the trending words.
    - The databases that can be used here is redis. The list of trending words can be extracted
    from redis to a User's UI. 
    - The trends can also be Geographic. 
    - For Passive Users, customized popular tweets can be made as a part of cron and sent out
    to those users, so that they can start tweeting again. This will use a notification 
    service which takes the information from the user service and sends the email to the 
    recipients. 

Topic: Facebook System Design 

Functional Requirements:
    - POST (Image, Text and Videos)
    - Like, Comment or Share on a post
    - Add Friends (Bi-Directional Relationship)
    - See Timeline
    - See a User's Post, Profile
    - Activity Logs for every Users

Non-Functional Requirements:
    - Read Heavy
    - Fast Rendering, Posting
    - Lag is Okay
    - Access Pattern -> High Usage for a specific period, followed by diminishing Activity
    - The Userbase, the Hardwares are and should be globally scalable.
    - Scale 
        - 1.7 Bil Daily Active Users -> 95% use mobile to access
        - 2.6 Bil Monthly Active Users -> 95% use mobile to access
        - Events/min:
            - 150K images
            - 300K statuses
            - 500K comments

Type of Users Classified for Facebook:
    - Famous
    - Active 
    - Live
    - Passive
    - Inactive

Design:
    - An User Onboarding API adds new users. It communicates to a User Service to update the 
    information which in turn is stored in a MYSQL Database. The information is also cached in a 
    Redis Database. When some query is required for user/users, it is queried to Redis first and 
    if the response is not received, then the query happens in the MYSQL DB. Also for user
    validity check and other notification services for the Users, the information is sent through 
    Kafka.
    - An Add Friend API adds new Friends. It connects to a Graph Service which provides friendship
    weightage, etc and the information is stored in a MySQL cluster. The same information regarding
    user details and their friends are also stored in a Redis DB. The Redis DB also contains the 
    following information:
        - User ID
        - User Details
        - Friends
        - User Types 
        - Relevance Tags : This is powered by an analytics service which provides valuable information
        on which friends are keen to watch which tags.
        - Last Access Time
    - The Add Post API communicates with a Post Ingestion Service. If the user posts a link, a 
    URL shortener service is used. If a User Posts a Video/Image, then the Access Pattern Analytics
    is invoked to verify the photo/videos engagements. Incase the engagement is high, it is stored 
    in Content Delivery Networks. If the engagement is low or has come down, then the content is 
    transferred from CDN to S3 bucket. If there is an engagement on an old content (Pic/Video), the 
    content is once again pulled from S3 bucket to CDNs.
    - The Textual Contents from the Post Ingestion Service are saved in Cassandra DB since there will 
    be a significantly large number of posts. Also these posts are put in KAFKA for doing classification
    Machine Learning on what label the post is. Once the ML performs the classification, the 
    classification result is then posted to a Post Processor Service, which takes the User ID and the 
    ML result. It then connects with the Redis DB and shortlists those friend profiles of the Users
    who has an interest on the post (Relevance Tags). 
    - A particular user can view either his someone else's profile or his own timeline. Incase he visits 
    someone else's profile, it calls the timeline service, and which in turn queries the posts service 
    which gets the data from cassandra and displays it back. 
    - If a User wants to check her/his timeline, it queries the TimeLine Service, which in turn queries
    User and Group Service to get the user's friends. The user's friend is fetched and then the contents 
    of those friends is fetched through Redis through timeline service which gets the data from redis. 
    The redis will only contain the information for the user's normal friends. To get the information 
    about the Celebrity friends a user has, the posts are queried through the post service which in turn 
    queries the Cassandra DB. Hence for normal friends, the posts are queried through Redis, while for 
    Celebrity friends, the users are queried through Cassandra. The posts of the famous users are then 
    persisted in Redis for few minutes along with the normal users and then once the time out takes place, 
    the Post Service is again used for querying Cassandra..  
    - To repopulate the timeline of the Live Users, the post processor which queries the users and group 
    service, gets information about the live users and for new feeds sends information through another
    Kafka Topic to a Live User Service, which in turn creates a Live Web Socket and feeds the live content
    through the User App/ Web.
    - Since Redis is an in memory DB, the older timeline that it contains needs to be populated to 
    a permanent storage. So a Cassandra DB is populated with users older time lines through an Archival
    service which contains the previous information. In case a user wants to see older timeline, than 
    once redis's timeline for the particular user is exhausted, it queries the Cassandra Time Line DB, 
    which contained time line in the format : {'u1':[e1,e2,etc],'u2':[e2,t4,etc]}. 
    - While creating the timeline cassandra db, we have to be careful with partitioning the Cassandra DB
    carefully as this might create a Hotspot. We must not partition by date range as the recent dates
    will create hotspots. We might want to create partition by user id or some more reasonable key.
    - The Like/Comment UI is powered by a Like Service and a Comment Service respectively. The
    likes of a particular post Id is stored generally as: 
    {postID:4,'originalUid':2, likeDetails:{likeUid:[2,3,4]}} in a Cassandra Db. The count of number of 
    likes is powered by Redis because for recent posts, the count of the likes for a particular post is 
    showcased real time by Redis. Whenever a Like is added, it gets posted into Kafka for analytics.
    - The Comment Service like Like Service is stored in Cassandra DB. The format is:
    {postID:4,originaUid:2,commentDetails:[{commentUid:1,comment:'Hi'},{commentUid:4,comment:'Hello'}]}. 
    This will not take the benefits of redis as the comments are not shown real time unless any
    specific user wants to check on the comments. That time it will be retrieved from the Cassandra DB.
    Whenever a Comment is added, it gets posted into Kafka for analytics
    - The Kafka Uses an Activity tracker Service and loads the data into Cassandra. It stores the 
    activity of the users' activity. For example if a user id 3 has liked uid 4's post, then a 
    document might get stored as: {activityUid:3,activity:'like',postId:4,OriginalUid:2}. 
    If a user wants to check someone else' activity, than it will be fetched from the activity cassandra 
    db. 
    - The Search Service gets the information from Kafka and it stores all the contents in an Elastic
    Search DB which in turn uses for searching and displaying the contents to the users. While a Person
    is searching for a particular information, that can be pushed into KAFKA and then the same can 
    be logged by activity tracker.
    - The output from the Kafka is used to run a Spark Streaming Job and thereby profile Users based on
    their likes to posts, their comments on certain posts, identify trends, etc. After User Profiling is
    completed, these users are marked through relevance tags and are fed only those posts from their 
    friends who upload posts, which is in synchrony to the users post's classification. 
    - The Graph weight job provides a weightage of friendship for a particular user with its list
    of users. Suppose a UId 1 has friends with UID 2,3,4 and 5. But UID 1 seems to like 90% of UID 5's 
    post, but seems to like only 10%, 20% and 40%  of UID 2, 3 and 4's post. Then the graph weight will
    give a must higher friend weight for UId1:UId5 w.r.t. UId2:UId3 and the rest. 
    - Various monitoring, alerting mechanisms needs to be integrated with the databases and clusters so
    that the non functional requirements are preserved.


Topic: YouTube/NetFlix/Prime/Hotstar System Design
Functional Requirements:
    - Upload Videos
    - User's HomePage + Search
    - Play Videos
    - Support all Devices
Non Functional Requirements:
    - No Buffering (Low Latency + High Availability)
    - User's Session Time (Will increase with Low Latency + High Availability and Good Recommendation 
    Service)

Incase there are i number of file formats in videos, j number of device's dimensions and k number of 
bandwidth available to stream the service, then the total number of combinations to be handled becomes 
very large, i.e., i*j*k. 

The factors to consider are:
    - Clients: Various Devices which has softwares to run the videos
    - Users: Various Human Users who will be watching the videos through the clients
    - Production Houses: Human users/ Companies who will be uploading the videos.

The Clients has Adaptive BitRate Streaming features, where in they may request video of a high quality
from the server and based on the amount of chunks of the content it receives from the server at a 
given time, it may vary the magnitude of the request. 

Production House Upload System Design:
    - Whenever an User/Production House wants to upload their videos, it is sent through to a Load
    Balancer to an Asset Onboarding Service(AOS). The AOS starts pulling the video from the source 
    and starts uploading it into S3. The metadata of the video that is being uploaded into S3 is 
    saved into a Cassandra DB, wherein all the relevant information like uID, S3 file path, 
    content name, content size are all stored. 
    - When the file is uploaded into Amazon S3, AOS sends out a Notification via KAFKA and its job
    ends.
    - The Content Processor Service then takes the information out from Kafka, and prepares
    the raw file that has been uploaded into S3. 
        - The CP Service first creates chunks of the original raw file through a file chunker
        - The Content Filter than analyses individual chunks for: Piracy, Nudity and Legal Clearances.
        - Once the above stage is passed, the Content Tagger then tags individual chunks and 
        providies a Thumbnail for individual chunks.
        - The Transcoders then converts individual chunks into multiple formats for the individual
        chunks
        - The Quality converted then converts the formats for individual chunks into multiple 
        quality. {chunk:1, attributes: {format: 'mp3', q: {quality:'high'} }}
        - Post that the video chunks are put into CDN
        - The information is sent through to KAFKA where in Spark Streaming is carried out and 
        all the information about the Contents, Formats, and Quality are then aggregated though 
        a MapReduce Functionality. 
        - While the File Chunker is sending back messages to KAFKA, an Asset Service takes the 
        details of the chunks and stores it into Cassandra DB.
        - Once the process is completed, the KAFKA sends out a notification service, through which
        the Uploader gets the notification, that the Upload was successful or not.

User System Design:
    -   A User Logs in through a User Device Login Flow which goes through a Load Balancer and hits
    the User Service. The User Service Sits on top of a MYSQL Database and also has a Redis Server 
    for caching mechanism. The User Service while retrieving the information of the user and the user's
    present log in details sends out to KAFKA for analytics, one of them can be to understand whether
    users are sharing password or not (different locations). 
    - After the Login, the user Reaches the Home Screen UI, from wherein they can search a video
    and that is powered by the search service. Few Analytics are made through an Analytics Service and
    they are sent to Kafka. 
    - Once a User Logs in and searches for a Video, the Video name gets connected to a Host Identity
    Service (HIS). The HIS searches for the Video in the Cassandra DB which is connected to an Asset
    Service which gives the information about the CDN the video resides on. 
    - The video might be located in a Main CDN or it might be located in a Local CDN. The job 
    of the HIS is to connect to the respective CDN for getting the content as searched by the user. 
    - The Stream Statistics Logger Service calculates the duration an user watches a Video and thereby 
    sends information to KAFKA which might be used to do Analytics on the like/dislike of certain 
    videos
    - For end to end Video Search from Uploader to Consumer, the Asset Service which has information
    about the files/videos and streams it on KAFKA the same is consumed by a Search Consumer Service
    which contains information about the age limit, etc about the video and stores the information 
    on an elastic Search. When a user queries through a search service, it queries the Elastic Search
    Cluster to retrieve the information about the movie. Now incase he wants to play the movie, 
    it connects to the HIS which goes to the Asset Service, which plays the Video through a particular
    CDN.
    - The tags which were being generated for individual chunks were sent to Spark Streaming though 
    Kafka. The Spark Streaming Service would do a reduce by operation and would aggregate the results
    of all the tags and classify the entire chunk of the video. It will then be sent to the KAFKA
    where by the document of the entire video gets updated. 
    - Cassandra is used to store all the metadata information about the videos, because it can store 
    massive amounts of data. For example, for 1 Billion YouTube Video, it will have roughly 1 Trillion 
    Chunks. Storing 1 Trillion Records is no Big Deal for Cassandra. 
    - Another Spark Analytics that can be used is segragating Thumbnails as per Users Like on the 
    Thumbnails. If there are more than one good thumbnail, it can be used to classify which user should 
    be shown which thumbnail, through Machine Learning App. 
    - Various Recommendation Services can be run for users, based on their watch time and then predicting
    how much rating he/she would have given to that movie of a particular genre. Incase the user would
    have given a high rating, he/she would be recommended another similar genre movie for viewing which
    he has not watched yet. 
    - Traffic Predictor ML can be run to load data into local/main CDNS through the same 
    spark streaming Analytic Services. 
    - CDN Writer Service looks for all the videos in S3 or Main CDNs and loads it into the local
    CDNs. The strategy to load is not load all the copies from S3/MDCDNs all at once into each
    LCDNS, but to decentralize the distribution process. Incase there are 20 chunks to be 
    distributed to each of the LCDNs from S3, then S3 will distribute 20/m copies in each of the 
    LCDNS and once all the 20 Chunks have been distributed to all the LCDNs combined, then the 
    LCDNs starts to distribute the chunks among themselves. This way, S3 does not become a single
    point of failure, also the price to pay for S3 becomes significantly lower.
    - Whenever a User searches for a specific movie which is very popular at the time of searching,
    the search travels from the user's laptop to the modem to the router to the ISP's router after 
    a certain hop and then it connects to the Streaming Platform. The Streaming Platform then 
    searches for the file in its CDN or in its S3 bucket. This entire process can give way to a lot 
    of latency. To minimize the Latency, any popular movie can be cached in the premise of the 
    ISP and when an User queries for the same, the ISP can directly serve the content to the user, 
    rather than reaching the Video Content Provider thereby decreasing latency. The Video content 
    provider for their Competitive Advantage may place the Hard Disk for caching to the ISPs with 
    no extra cost for the ISPs. 

Topic: Zoom/ Webex/ Whatsapp/ FB Video Call:

Functional Requirements:
    - 1 on 1 calls
    - Group calls
    - Audio/ Video/ Screen Share
    - Record

Non Funtional Requirements:
    - Super Fast Streaming as it is a Live Call
    - High Availability
    - Data Loss is Okay

In a TCP world, all the information that are sent from Client to The server is in the form of
a Packet with a Packet Number. TCP tries to be a lossless protocol. It tries to the best of its 
abilities to make sure that there is no data loss between a client and a server. And whereever it
figures out that there is a potential data loss, i.e., it did not get an acknowledgement for a 
particular packet, it tries to replay that. Also on the server side, it tries to arrange all of the 
packets in the same order in which they were supposed to be based on the packet number, and then 
sends it back to the application. 

Congestion Control is another activity performed by TCP. It makes sure it does not send too many 
packets at one go therefore not overwhelming the network with a lot of packets. Also header
size of TCP is 20 bytes which is a considerable size.

In an UDP world, the UDP keeps on sending the payload, irrespective of whether the server is getting
it or not. Hence data transfer in UDP is significantly less as compared to in TCP. The header size
of UDP is 8 bytes, which is significantly lesser than that in TCP. UDP is therefore a lossy 
protocol, but it will be faster than a TCP. 

Only the Video transfer will happen through UDP, while all the other transfers like call 
establishing, etc will happen with TCP.

The calls are established with U1 and U2 through Web Socket Handlers when both parties are online,
else some notification is provided to the caller about the user's offline status. 

Web Socket Handlers usage increases Network Latency. Hence to combat that, call with UDP is 
established, where in the public IP address of both the parties are only needed to instantiate 
the connection. 

Incase of Zoom, there is a connector service, where in when user wants to join a bridge, it informs
the connector service about its IP address, while the other user also informs the same connector
service about its IP address, and if the bridge id is common for both the users, than an UDP 
connection is established.  

For devices connected through IPV4, they are internally connected through Private IP addresses. 
Hence when they wants to get connected to the connector services, they must get their public
ip address from their ISP providers in the format A.B.C.D:port, where in A.B.C.D is the IP address
of the router, and the port is the port number of the device that it is connected with the 
router. 

Incase of symmetric NAT, where a call can be established by only the connectors and not through
the user devices (Connector can only pass on information from A.B.C.D:port1 to A.B.C.D:port1) while
User 1 whose public IP was A.B.C.D:port1 cannot talk to User 2 directly whose public IP was 
A.B.C.D:port2.

To get through the above scenario, a call server model is called on through a WEBRTC protocol which
is Web Real Time Communication. 

Web Socket Handler is known as Signalling, Connector Server is called as STUN server and 
Call server is called TURN. 

For Group Calls, the the service is divided into two parts:
    - Small Group (User Size <=5 Members)
    - Large Group (User Size > 5 Members)

When a User has to communicate information in a group of n member size, he has to communicate the
same information to (n-1) users, thereby eating a lot of bandwidth. To get away with this problem,
the call server is utilized, where in the hosts bandwidth is saved.

Encryption happens end to end at the start of the call. Hence any hacker cannot intercept the call
in between. 

High Level System Design:
    - Every User talks with a Web Socket Handler and the information about user:host is kept by 
    web socket manager. The Web Socket manager sits on top of redis and a persistent disk - to make it 
    fault tolerant.
    - The call initiation activity is performed by a Signalling Service. It uses the user service 
    in the backend, to get the information about the user datails of the called and the calling party
    and then decides whether the call should happen. If the call can happen, then it informs the 
    web socket handler of the calling party, whether the called party can answer or not. 
    - Also during call establisment, the Connector STUN server is used to fetch the public IP details
    of the called and the calling party. 
    - Incase of a Symmetric NAT, a STUN Server or a Call Server is used to orchestrate the 
    communications with both the parties. 
    - During a Call if the bandwidth capacity changes, then the communications happens through the 
    web socket handler and the signalling service. 
    - While the Change in bandwidth happens during a call, the information is put out by the 
    Signalling Service to KAFKA to an Analytics Engine. 
    - For recording purpose, the TURN server while sending the packets to the recipient users, also
    saves the same packets through a Logger Service in a Distributed File System like Hadoop or S3. 
    - When a call gets terminated, the Signalling Service send out an event through KAFKA to to a 
    File Creator Service which is also dealing with the aggregation of the packets of information
    from Logger Service before putting it in S3. Once the File Creator Service gets the information 
    about the meeting's termination status, it accumulates and stores the entire packet into one 
    in S3. 
    - The file Creator Service after storing the information in S3, would use a Notification service
    to inform the users about the call recordings. 
    - For a Group Conversation a Transcoding Service is used by the Call Server. Incase there are
    three users U1, U2 and U3 and U3 has a low internet bandwidth compared to U1 and U2. In this
    scenario, the Call Server will take the help of the trancoder service and lower the bandwidth 
    of the HD videos coming from U1 and U2 and going to U3. The video which is arising from U3 are 
    sent in the original Low Definition format from U3 to U1 and U2. 
    - Incase a peer to peer conversations gets updated to a Group Conversation, than the system should
    be intelligent enough to decipher and transfer the call from the Connetor-STUN server to Call
    Server-TURN server by taking the information from the signalling service. 

For a Live Stream Event, when ever an Information Packet is sent to a Call Server, the Video 
Information is first broken down into specific codes which can be used by various devices and passed
on to different call servers based on Geography. Now these call servers will have packet info 
of various codec and they are further distributed into call servers based on a particular codec
and they in turn provides content directly to the users devices. This Call servers must be located 
as near possible to the end users consuming these feed. 

Topic: Google Maps:

Functional Requirements:
    - Identify Roads
    - Distance & ETA b/w 2 points
    - Plugable
Non Functional Requirements:
    - High Availability
    - Good Accuracy
    - Not too Slow
    - Scale:
        - 1 Billion Monthly Active Users (Roughly 5-10 Billion Requests per month)
        - 5 Million Companies 

Ideology of Dynamic Programming is required to solve such use cases:

Segment is a small area where we can easily operate on with. We should be in a position to calculate
whether a user falls under a certain segment. 

There will be a directed weights between two points as Time Taken will be different to cover two
directions. 

There will be a calculated edge between two points which will say that the shortest path between 
two points is 2000 seconds and the points are A-B-C.

To handle distance between points which is not a junction, we calculate the nearest distance of the
junctions between the sourcw and also the nearest distance of the junctions from the destination. Then
we calculate the nearest distance from the source junctions and the destination junctions crosses. 
Once we identify the nearest junction distance, we then add the distance from the points to the 
juntions (both source and destinations) and get the nearest distance. 

To traverse inter segments, we use the shortest distance of the two exits from a segment to connect
two segments.

For travelling intercity, there might be thousands of segments, and that will be twoo complicated
to run at runtime. Hence we create mega-segments and we calculate the distance of the start point 
to the exit of the megasegment to the entry and and exit of the subsequent mega segments.  

Weights to consider for an Edge:
    - Distance between two points
    - Under Normal set of Conditions, time to reach from A to B
    - Average Speed

A good design should not entail addition of new features, plus each change should be small enough.
Hence the weghts must not contain traffic and weather information as predictions. Those can be 
attributes. 

Incase previous users time to reach Point A to Point B takes t mins on an average, then traffic 
data may not be used for predicting average speed. Incase users time is not available, then traffic
can be used for predicting the average time. 

Suppose initially it was recommended to follow a path -> SE1->A->B->C->SE2 to reach from point SE1->SE2
based on the weights of the Time. Now incase the time increases significantly fpr path A and B based on 
the activities of the real time users, than the shortest path between A and SE2 is calculated which may
or may not include point C. 

Architecture System Design to keep track of Users Movement who is not using Map Service extrinsically:
    - When ever location is turned on in a device, location ping is sent from the device every few seconds
    - If the user is static, the location ping may be sent considerably at an increased frequency vis-a-vis
    while an user is in transit. 
    - Each active device is connected to a Web Socket Handler and the information about the device:handler
    configuration is preserved by Web Socket Manager. The Web Socket Manager stores the information in 
    Redis.  
    - The location of the users are captured by location service which keeps the information in a cassandra
    database. The location pings are also queued through Kafka. 
    - Once the location of users comes from Kafka into Spark Streaming, it is used for the following
    services:
        - Roads Job (New): Incase there is a movement by a lot of people over a segment not captured
        in the map db earlier, than that road can be added by sending the information back to Kafka and 
        thereby using the Map Update Service which uses the Graph Processing Service and updates in into
        Cassandra. The new road added will be part of the segment which will be used to calculate the time 
        and distance between points and potentially become the shortest path.
        - Average Speed Job: The average speed of the users over a particular path are calculated and are sent
        via Kafka to a Traffic Update Service and this can be utilized in a Graph Processing Service and 
        finally into Cassandra. This average speed can be a deciding factor on selecting that road as 
        a part of a segment while calculating the shortest path. 