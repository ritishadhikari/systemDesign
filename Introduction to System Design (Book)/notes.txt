Scalability: It is the capability of a system, process or network to grow and manage increased demand. 
Any distributed system that can continously evolve to support the growing growing of work is considered
scalable.

Horizontal Scaling: It means that you scale by adding more servers into your pool of resources. Ex: Cassandra
and MongoDB

Vertical Scaling: vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc). Ex: MySQL

Reliability: A distributed system is considered reliable if it keeps delivering its services even when one or 
several of its software or hardware components fails. A distributed system achieves reliability through the
redundancy of both the software components and data.

Availability: Availability is when a system remains operational to perform its required function in a specific
period. If a system is reliable, then it is available. However, if it is available, it is not necessarily 
reliable.

Two Standard measures of efficiency are:
    - Response Time (latency): Denotes the delay to obtain the first item.
    - Throughput (Bandwidth): Number of items delivered in a given unit (eg. a second) 

Serviceability or Manageability:  It is the simplicity and speed with which a system can be repaired or
maintained. Early detection of faults can decrease or avoid system downtime. 

Capacity Estimation and Constraints:
    - Traffic Estimates
    - Storage Estimates
    - Bandwidth Estimates
    - Memory Estimates

Incase we anticipate storing billions of rows, and we don't need to use relationships between objects, a 
NoSQL DB is a better choice. A NoSQL choice would also be easier to scale. 

Data Partition is required to store information about billions of URLs. Partitioning can be carried out via:
    - Range Based Partition
    - Hash-Based Partition
    - Caching of frequently used data

When the Cache is full, and we want to replace a data which is the most recent one, we can chose the Least 
Recently Used (LRU) policy, where in we discard the least recently used data first from the
cache memory.

A Load Balancing mechanism with the Round Robin approach takes out the server which is dead and stops 
sending any traffic. But a problem with Round Robin is that, it won't consider server load. Therefore if a 
server is overloaded or slow, the LB will not stop sending request to that server. Hence a more intelligent
solution is required.

In an App like Instagram, we need to create an index on the photoId, creationDate in the photo table, since 
we need to fetch recent images first.

We can plan to have dedicated servers for reads and different servers for writes so as to ensure that 
uploads don't hog the system.

Rather than keeping data by userID to create shards, we must create unique dataID and then place those 
dataID%100 in specific shards. When we do this, we do not place all the users data in a specific shard and
hence do not congest it incase the userId happens to be an Influencer.

A key generating DB can be a single point of failure. One work around could be defining two such databases, 
one generating even-numbered IDs and the other odd-numbered. Alternately we can implement a key generation
scheme.

In a HTTP Long Polling, clients can request information from the server to expect that the server may not
respond immediately. If the server has no new data for the client when the poll is recieved, instead of 
sending an empty response, the server holds the request open and waits for response information to become 
available. Once it does have new information, the server immediately sends the response to the client, 
completing the open request. Upon receipt of the server response, the client can immediately issue another 
server request for future updates. This gives a lot of improvements in latencies, throughputs and performance. 
However, the long polling request can timeout or receive a disconnect from the server. In that case, the client
has to open a new request. 

For instant messaging services, we can HBase which is a column-oriented key-value NoSQL database that can 
store multiple values against one key into multiple columns. 

HBase groups data togather to store new data in a memory buffer, and once the buffer is full, it dumps the 
data to the disk. This storage method helps store a lot of small data quickly and fetching rows by the key
or scanning ranges of rows. 

HBase is also an efficient database to store variable-size data which is also required in instant messaging
services.

In a Group Chat, a group chat object is identified by GroupChatID and will maintains a list of people (UserId)
who are part of that chat. '

To have push notification in our system, we would need to set up a Notification server, which will take the messages
for offline users and send them to the mobile manufacturer's push notification server which will send them to 
the user's device. 

In a file storage system like DropBox and others, internally files can be stored in small parts or chunks (say 4MB). 
This can provide many benefits, e.g., all failed operations shall only be retried for smaller parts of a file. If a 
user fails to upload a file, then only the failing chunk will be retried. 

In a system like Google Drive, we would need the following servers:
    - Block Servers which will work with the clients to upload/download files from cloud storage
    - Metadata servers will keep metadata of files updated in a SQL or NoSQL database
    - Synchronization servers will handle the workflow of notifying all clients about different changes 
    for synchronization.

The Message queuing service supports asynchronous and loosely coupled message-based communication between distributed
components of the system. The Message Queing Service should efficiently store any number of messages in a highly
available, reliable and scalable queue. 

In dropbox, Message Queing Service will implement two types of queues in our system. The Request Queue is a global
queue, and all clients will share it. Clients' requests to update the Metadata Database will be sent to the Request 
Queue first. From there, synchronization service will take it to update the metadata. The response queue that 
corresponds to individual subscribed clients are responsible for delivering the updated messages to each client. 
Since a message will be deleted from the queue once received by a client, we need to create separate Response queues
for each subscribed client to share update messages.


Deduplication of incoming/outgoing data through hashing can be done via:
    - Post Process Deduplication
    - InLine Deduplication. Gives Optimal Network and storage usage as hash calculations are done real time.


Partitioning can be carried out in the following way:
    - Vertical Partitioning: Different Businesss Specific Tables in Different databases
    - Range Based Partitioning: Separate partitions based on the first letter of the file path
    - Hash Based Partition: Separate Partitions based on the hash of the FileID of the File Object. 


Glossary:

SQL vs No SQL: Relational databases store data in rows and columns. Each row contains all the information about one
entity, and columns are all the separate data points.

No SQL Databases have different data storage models. The main ones are key-value, document, graph and columnar. 

SQL can be scaled vertically(CPU, memory, etc) which can be expensive while NoSQL can be scaled horizontally by 
adding new servers which can be more cost effective. 

SQL is ACID compliant while NoSQL is not ACID complaint.

In Consistent Hashing, when the hash table is resized (e.g., a new cache host is added to the system), 
only k/n keys  need to be remapped, where k is the total number of keys and n is the total number of servers. 
While in a caching system using the 'mod' as the hash function, all keys need to be remapped. 

In Consistent hashing, objects are mapped to the same host if possible. When a host is removed from the system, 
its objects are shared by other hosts. When a new host is added, its takes its share from a few hosts without 
touching other's shares. 

WebSocket provides Full Duplex communication channels over a single TCP connection. It provides a persistent connection 
between a client and a server that both parties can use to start sending data at any time. The client establishes a 
Websocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and
client can exchange data in both directions at any time. The Websocket protocol enables the communications between a 
client and a server with lower overheads, facilitating real-time data transfer from and to the server.

In a Server-Sent-Events(SSEs), the client establishes a persistent and long term connection with the server. The server
uses this connection to send data to a client. 1. Client request data from a server using regular HTTP. 2. The requested
webpage opens a connection to the server. 3. The server sends the data to the client whenever there's a new information
available. SSEs are best, when we need real-time traffic from the server to the client or if the server is generating 
data in a loop and send multiple events to the client.

Various Load Balancing Algorithms:
    - Least Connection Method: Directs traffic to the server with the fewest active connections. 
    - Least Response Time Method: Directs traffic to the server with the fewest active connections and the lowest
    average response time
    - Least Bandwidth Method: Directs traffic to the server serving the least amount of traffic measured in Mbps.
    - Round Robin Method: Cycles through a list of servers and sends each new request to the next server. When it 
    reaches the end of the list, it starts over at the beginning. 
    - Weighted Round Robin Method:  Each Server is assigned weights depending on their processing capacities, and 
    servers with higher weights receive new connections before those with fewer weights. 
    - IP Hash: A hash of the IP address of the client is calculated to redirect the request to a server.

Redundant Load Balancers: The load balancer can be a single point of failure. To overcome this, a second load balancer can
be connected to the first to form a cluster. Each LB monitors the health of the other and, since both of them are equally
capable of serving traffic and failure detection, the second load balancer takes over in the event the primary load balancer
fails. 

Cache Invalidation Techniques:
    - Write-through Cache: Data is written into the cache and the corresponding database simultaneously. Ensures complete
    data consistency between cache and the storage. Disadvatage is higher latency for double write operations. 
    - Write-around Cache: Data is only written directly into permanent storage bypassing the cache. Advantage is it 
    reduces the cache being flooded with write operations. Disadvantage: Read request for recently written data will 
    create a cache miss and must be read from slower backend storage and hence higher latency. 
    - Write-Back Cache: Data is written to cache alone and completion is immediately confirmed to the client. The 
    write to permanent storage is done after specified intervals or under certain conditions. Advantage
    is low latency and high throughput for write-intensive applications. Disadvantage: More prone to data loss.

Cache Eviction Policies: 
    - First In First out
    - Last In First out
    - Least Recently used (LRU)
    - Most Recently Used (MRU)
    - Least Frequently Used (LFU)
    - Random Replacement (RR)